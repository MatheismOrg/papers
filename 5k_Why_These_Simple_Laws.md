# **Why These Simple Laws? Deriving Physics from Mathematical Necessity**

**Gary Bernstein**

matheism.org

## **Abstract**

I establish that existence is identical to mathematical consistency through a priori argument, then derive consequences for physics. The pattern-randomness dichotomy proves exhaustive: anything intelligible exhibits either discoverable patterns or apparent randomness, both inherently mathematical. Combined with the Identity of Indiscernibles applied to isomorphic structures, this establishes that reality *is* mathematical structure, not merely described by it; the math is the territory. From this foundation, the Level IV multiverse follows deductively: all self-consistent mathematical structures necessarily exist, since any filter is itself a consistent structure, all filters exist, including the null filter that excludes nothing. Only computable structures have defined measure and contribute to predictions. The measure problem (which structures should observers expect to inhabit?) is addressed by the Induced Parsimony Postulate (IPP), building on Schmidhuber's (2000) algorithmic probability framework: P(S) ∝ 2^(−K(S)), where K(S) is the Kolmogorov complexity of its generating rules. Since reality is structure, this measure is not imposed but discovered: algorithmic probability is inherent to structure-space. IPP explains cosmological fine-tuning, the success of Occam's Razor, and why physical laws take variational form. It generates testable predictions: the final theory of physics should have near-minimal generative complexity among observer-supporting structures, and outstanding puzzles (quantum gravity, dark matter, dark energy) will have simple solutions. Unlike Tegmark's Mathematical Universe Hypothesis, which posits mathematical existence as cosmological conjecture, this framework derives it from logical necessity.

**Keywords:** mathematical universe, structural realism, multiverse, Kolmogorov complexity, fine-tuning, measure problem

---

## **1\. Introduction**

Reality must be mathematical structure. This is not hypothesis but derivation.

The argument is simple: anything intelligible exhibits either discoverable patterns or apparent randomness. Patterns are regularities expressible through mathematical relations. Randomness is characterized via probability distributions and algorithmic incompressibility. Both are inherently mathematical. There is no third option. This exhaustive dichotomy establishes that whatever exists must be mathematical structure.

From this foundation, striking consequences follow. If existence is mathematical consistency, all self-consistent structures exist. The Level IV multiverse emerges as deductive consequence rather than cosmological conjecture. This resolves the grounding problem facing Tegmark's (2008, 2014\) Mathematical Universe Hypothesis: he posited mathematical existence; we derive it.

But the multiverse raises a new puzzle: the measure problem. Among all structures, which should observers expect to inhabit? Only computable structures have defined algorithmic probability; non-computable structures are included for logical completeness but contribute nothing to predictions. Without a measure, we cannot make predictions. Certain naive measures yield catastrophe (Boltzmann brains dominating). This paper proposes the Induced Parsimony Postulate (IPP): structures are weighted by algorithmic probability, P(S) ∝ 2^(−K(S)), where K(S) is Kolmogorov complexity. IPP is not arbitrary but uniquely motivated by information-theoretic considerations. Moreover, since reality *is* structure, the measure is not chosen but discovered: algorithmic probability is the natural measure over structure-space, not an external imposition.

The framework explains fine-tuning (simple laws dominate), variational form of physical laws (minimal generative complexity), and Occam's Razor success (simplicity tracks truth). It generates testable predictions: the final theory should have near-minimal complexity among observer-supporting structures.

The position developed here (call it *mathematical monism*, or *matheism*) completes Ontic Structural Realism (Ladyman and Ross 2007, French 2014). Standard OSR describes structure as reality's sole constituent but does not explain *why* structure exists. The pattern-randomness dichotomy closes this gap.

---

## **2\. The Foundation: Mathematical Structure as Necessary**

### **2.1 The Pattern-Randomness Dichotomy**

Any intelligible reality must exhibit one of two characteristics: discoverable patterns or apparent randomness. This dichotomy is exhaustive.

Patterns are regularities expressible through mathematical relations. Randomness is itself mathematically characterized via probability distributions and algorithmic incompressibility (Kolmogorov complexity). There is no third option: a phenomenon is either patterned (mathematically describable regularity) or random (mathematically describable via stochastic properties). Both categories are inherently mathematical.

For something to exist and be distinguishable from nothing, it must possess determinate features. Properties that are neither patterned nor random would be indeterminate, indistinguishable from nonexistence. Existence requires determinacy; determinacy requires pattern or randomness; both are mathematical. Therefore, existence itself is mathematical structure.

This dissolves the ancient notion of substance. Traditional metaphysics posited substance as the bearer of properties, the "stuff" underlying attributes. But what is substance apart from its properties? If we strip away all relational and qualitative features, nothing determinate remains. Substance without properties is indistinguishable from nothing. And properties are relations: mass is gravitational coupling, charge is electromagnetic interaction, spin is transformation behavior under rotation. Physics has already completed this dissolution: quantum field theory describes particles as excitations of fields, fields as sections of fiber bundles, forces as spacetime geometry or gauge connections. At no point does "stuff" appear. Structure goes all the way down.

Four potential counterexamples deserve consideration:

*Ontic vagueness*: indeterminate properties still require relational boundaries to differ from other vague properties. These boundaries constitute structure.

*Primitive dispositions*: a 'power' is defined by its conditional manifestation (If X, then Y). This If-Then structure is a logical function. Dispositions are mathematical functions renamed.

*Hypercomputation*: structures exceeding Turing-computability are still mathematical objects describable via oracle Turing machines.

*Atomless gunk*: infinitely divisible structure is still relational structure.

All alternatives reduce to patterned or random mathematical forms. The dichotomy is logically exhaustive.

### **2.2 From Isomorphism to Identity**

Any relational structure is isomorphic to at least one mathematical object. An isomorphism is a structure-preserving mapping: it preserves all relations while potentially changing labels. Consider: what distinguishes a physical relational structure from an isomorphic mathematical structure?

Not their relationships: identical by definition of isomorphism. Every connection in one corresponds to a connection in the other. Not their causal powers: causal powers supervene on relational structure. If A causes B, this relationship is part of the structure; isomorphic structures have identical causal patterns. Not observable properties: all reduce to relational structure. Color is wavelength (relation to electromagnetic spectrum). Hardness is bonding strength (relation between atoms). Temperature is molecular motion (relation between kinetic energy and degrees of freedom).

The only candidate is non-relational 'substrate': primitive stuff bearing properties without being constituted by them. This is what philosophers call 'bare particulars': something that has properties but is not defined by them. But substrate faces decisive problems:

**First**, physics has eliminated it. Quantum field theory contains no objects with intrinsic properties independent of relational role. The electron's mass (0.511 MeV/c²), charge (-e), and spin (½ℏ) are not properties attached to underlying electron-stuff. They *are* how electrons relate to fields, particles, and measurements. Remove these relations, and nothing remains. The Standard Model is purely relational: gauge fields, coupling constants, symmetry groups, with no underlying substance.

**Second**, substrate does no explanatory work. Consider two systems with identical structure but hypothetically different substrates. They behave identically in all observable ways. The substrate makes no difference to anything we can measure, predict, or explain. It is explanatorily idle: a violation of parsimony. Moreover, any substrate must have determinate features (else it is nothing), and determinate features constitute structure. By the pattern-randomness dichotomy, substrate *is* structure.

**Third**, the Identity of Indiscernibles. If two things share all relational properties, what non-question-begging criterion distinguishes them? Haecceities (primitive 'thisness') are substrate by another name, invoked precisely when all relational properties match. The haecceitist must specify what makes *this* haecceity differ from *that* one. Any answer appeals to distinguishing features, which are properties. No answer leaves the distinction primitive and unexplanatory.

The standard counterexample (Black's (1952) symmetric universe with two qualitatively identical spheres) fails upon examination. The spheres differ *relationally*: each is 'the one two miles from the other.' Their distinctness is constituted by mutual spatial relations within the complete structure. More significantly, Saunders (2006) demonstrates that even putatively identical quantum particles are *weakly discernible* through irreflexive relations: particle A stands in the relation 'has opposite spin to' particle B, a relation neither particle bears to itself. Weak discernibility suffices for numerical distinctness.

Two examples illuminate different aspects of this identity:

*Chess* shows substrate irrelevance: implemented in carved wood, computer memory, or pure abstraction, no chess property differs (no legal move, strategy, or checkmate). The game is the relational structure; the wood is merely one access mode. Burning the wood destroys a token, not the type. This shows the substrate does no work.

*A perfect map* shows structural identity: imagine a map so detailed it captures every feature of the territory at 1:1 scale, including the map itself, recursively. At what point does "map of territory" become "territory"? If the map preserves all relational structure perfectly, the distinction dissolves. The map does not *represent* the territory; it *is* the territory, accessed differently. This is the relationship between mathematical structure and physical reality. The math is the territory.

**Therefore**: By Leibniz's Identity of Indiscernibles, physical reality and mathematical structure are identical. Reality *is* mathematical structure, not an instantiation of it in some separate substrate.

### **2.3 Observers Require Patterns**

A crucial corollary: observers are logically constrained to patterned realities. An observer is a subsystem that (i) persists as stable structure across time and (ii) supports reliable counterfactual-tracking states. Both require systematic relationships: memory traces encoding past events, causal chains connecting configurations, stable structures that don't randomly dissolve.

Any subsystem satisfying these conditions exhibits pattern: its states must be describable by rules shorter than complete enumeration. No observer can exist in brute-random structure where each moment is algorithmically independent.

This is not probabilistic but conceptual: 'observer' and 'brute randomness' are mutually incompatible categories. When we ask 'why do we observe a patterned universe?', we ask a necessarily self-answering question. Observation presupposes patterns.

---

## **3\. The Level IV Multiverse**

We have established that existence is mathematical consistency. From this, a striking consequence follows: all self-consistent mathematical structures necessarily exist.

Consider the alternative. Suppose some consistent structures exist while others don't. What determines which? Some principle G must filter structures into 'existing' and 'non-existing' categories. But G is itself a rule, a specification, a structure. If G is consistent, it exists by our criterion. If G exists, we must ask: what determines that *this* G obtains rather than alternative G'?

We face regress. Either: (a) some G is brute (it just happens to be the filter, with no explanation), violating the Principle of Sufficient Reason and introducing arbitrary contingency at the foundation of reality; (b) regress continues infinitely, with no ultimate explanation; or (c) there is no filter: all consistent structures exist.

Option (c) is the only non-arbitrary, non-regressive solution. Every possible filter G is itself a consistent mathematical structure, so every filter exists, including the 'empty filter' that excludes nothing. There is no privileged selector because all selectors exist. Existence is synonymous with mathematical consistency. A structure 'exists' if and only if it is free from internal contradiction. This is the Level IV multiverse: the ensemble of all self-consistent mathematical structures.

This vindicates a qualified Principle of Sufficient Reason. Explanatory chains terminate at mathematical necessity rather than regressing infinitely or stopping arbitrarily. Mathematical consistency is self-explanatory in the way 'a circle is round' is: further 'why' questions become incoherent. Consider: 'Why is 2+2=4?' This question misunderstands mathematical truth. 2+2=4 follows from definitions and axioms; it is not arbitrary fact requiring external explanation but necessary relationship within self-consistent structure.

The physical/abstract distinction dissolves. 'Physical' names structure experienced from within; 'abstract' names structure viewed from outside as pure form. The felt concreteness of tables and chairs is an indexical fact of being embedded in structure, not evidence of special ontological category. Asking 'why is my universe real rather than merely mathematical?' is like asking 'why am I here rather than there?', presupposing an external vantage that does not exist. The hardness of a rock, the heat of a flame, the pain of a stubbed toe are all patterns of information processing within structure, not evidence of non-mathematical "physical stuff." Qualia are what mathematics feels like from inside.

This dissolves Benacerraf's (1973) dilemma about mathematical knowledge. How can we know abstract objects if they are causally inert? Answer: we know mathematical truths because we *are* mathematical structures, subpatterns with direct acquaintance rather than requiring causal contact.

This position resembles but differs from Lewis's (1986) modal realism. Lewis posits concrete possible worlds as real as our own, distinguished by spatiotemporal isolation. The present view posits mathematical structures, not concrete worlds. Lewis faces the incredulous stare: why believe in infinitely many concrete universes? Mathematical monism faces no such burden. Mathematical structures are not additional concrete entities but the very fabric of consistency itself. The 'existence' of the Mandelbrot set requires no ontological extravagance; it follows from the definition. The multiverse here is not a collection of spatiotemporally isolated physical universes but the totality of self-consistent mathematical forms.

---

## **4\. The Measure Problem and IPP**

### **4.1 The Induced Parsimony Postulate**

The multiverse resolves fine-tuning in principle but raises a new question: among all consistent structures, which should observers expect to inhabit? Without a measure, we cannot make predictions. Worse, certain naive measures yield catastrophic conclusions: under uniform measure over observer-moments, Boltzmann brains vastly outnumber structured observers.

A crucial point: Since reality IS structure (proven via the dichotomy), the measure is not chosen but discovered. We are not imposing a measure on reality from outside; we are identifying the measure that structure-space inherently has. Algorithmic probability is the natural measure over computational/structural space, arising from the combinatorics of description length. This transforms the measure problem from "which measure should we choose?" to "what measure does structure-space have?" The answer is algorithmic probability, for the same reason the natural measure over integers weights small numbers more heavily: there are more ways to specify them.

I propose:

**IPP:** P(S) ∝ 2^(−K(S))

where K(S) is the Kolmogorov complexity of S: the length of the shortest program generating the structure's rules when run on a universal Turing machine.

This approach builds on Schmidhuber's (1997, 2000, 2002\) algorithmic theories of everything, which first proposed that all computable universes exist and are weighted by algorithmic probability. Schmidhuber further refined this with the Speed Prior, incorporating computation time alongside program length. The present framework differs in two respects: first, it includes all self-consistent mathematical structures, not only computable ones, since consistency is the criterion established by the pattern-randomness dichotomy; second, it provides philosophical grounding for *why* algorithmic probability governs measure, rather than positing it as a starting assumption. Tegmark's (2008) Mathematical Universe Hypothesis shares our conclusion but not our derivation; Schmidhuber's framework shares our measure but not our scope. The present work unifies both: deriving mathematical existence a priori, then applying algorithmic probability as the natural measure over structure-space.

In practice, non-computable structures have undefined K, so IPP assigns them no determinate measure; only computable structures contribute to predictions. The inclusion of all consistent structures is for logical completeness, as the pattern-randomness dichotomy does not exclude them a priori. More precisely: non-computable structures would require oracle machines, which depend on oracles encoding infinite brute facts (e.g., which programs halt). If there were a pattern, the oracle would be computable. So oracles are maximally random, with K \= ∞, yielding measure 2^(-∞) \= 0\. Non-computable structures exist but have measure zero. This derives Schmidhuber's restriction to computable structures from the measure itself, rather than imposing it by fiat.

Note that simple, finitely specifiable laws can generate non-computable phenomena as outputs (e.g., undecidable spectral gaps, chaotic systems requiring infinite precision). The laws have low K; some consequences may not. The measure weighs structures by the complexity of their generating rules, not their outputs.

**Critical distinction:** K(S) measures *generative* complexity (program specifying laws), not instance complexity of states. A universe with simple laws can produce vast complexity. The Mandelbrot set has K(S) of \~50 characters but infinite fractal detail. Similarly, simple physical laws (Standard Model plus general relativity, \~10³ bits) produce the observable universe's vast complexity.

Four justifications support IPP:

**Logical-space:** In hypothesis space, simpler hypotheses make fewer arbitrary commitments. A theory specifying 10 parameters is compatible with fewer worlds than one specifying 5\. Simpler structures are less constrained, hence 'larger' in logical space.

**Combinatorial:** Among programs of length ≤ n bits, shorter programs vastly outnumber longer ones. There are 2^k programs of length k, so length-k programs are exponentially more numerous than length-(k+1) programs. If a universal Turing machine generates programs randomly, shorter programs appear exponentially more frequently. This underlies Solomonoff's (1964) foundation for inductive inference; IPP extends it from epistemology to ontology.

**Coherence:** IPP makes observed physics non-miraculous. Physical laws exhibit striking simplicity: the Standard Model Lagrangian fits on a t-shirt; general relativity derives from a one-line action principle. If all structures existed with equal measure, simple laws would be vanishingly unlikely among observer-supporting structures. IPP explains why simplicity prevails.

**Invariance:** Algorithmic probability is the unique measure invariant under computable transformations up to multiplicative constants. While K(S) depends on choice of universal Turing machine, different machines yield complexities differing by at most an additive constant (the invariance theorem). The exponential relationship 2^(−K) dominates these additive shifts.

This uniqueness deserves emphasis. The critic asking 'why algorithmic probability rather than some other measure?' faces a burden: *produce* an alternative satisfying comparable desiderata. Lebesgue measure is undefined over the relevant space (no natural uniform distribution over an infinite ensemble with unbounded cardinality). Uniform measure over bounded complexity is arbitrary at the bound. Measures favoring complexity face the Boltzmann brain catastrophe. Algorithmic probability is the only measure with independent mathematical motivation (Solomonoff induction), the only measure avoiding known pathologies, and the only measure explaining the observed success of parsimony in science.

### **4.2 Occam's Razor Grounded**

IPP provides something remarkable: a grounding for Occam's Razor. Traditionally, parsimony is methodological preference without clear justification for why simpler theories are more likely true.

IPP transforms this. Since simpler structures dominate the measure, simpler theories are more likely true because they describe more probable structures. Parsimony is not aesthetic preference but theorem about reality's structure.

This explains the mysterious success of parsimony in science. Newton's laws, Maxwell's equations, Einstein's field equations, the Standard Model Lagrangian all exhibit simplicity despite describing enormous complexity. If simplicity had no connection to truth, this track record would be miraculous. IPP explains it: scientists have been unconsciously tracking algorithmic probability, favoring theories describing high-measure structures.

### **4.3 Three Types of Probability**

Three distinct concepts travel under "probability" in this framework; conflating them breeds confusion.

*Quantum probability* concerns credences within a branch. If Many-Worlds is correct, all measurement outcomes occur; the Born rule governs rational expectation about which branch contains one's continuation. This is epistemic: indexical uncertainty, not ontological indeterminacy.

*Measure-theoretic weight* concerns the distribution over mathematical structures. Simpler structures have higher algorithmic probability. This is not about which structures "get created" (all exist timelessly) but about their relative weight in structure-space.

*Anthropic constraint* concerns which structures can contain observers. Combined with measure, this yields predictions: observers should expect to find themselves in high-measure, observer-supporting structures.

These three probabilities are related but not identical. Quantum probability operates within a structure; measure-theoretic weight operates over structures; anthropic constraint filters structures relevant to prediction.

---

## **5\. Consequences and Predictions**

### **5.1 Fine-Tuning Resolved**

Physical constants appear calibrated for complexity. IPP explains: among observer-supporting structures, those with simple laws dominate. Simple laws produce orderly universes where constants permit complexity. Fine-tuning is what simplicity looks like from the inside.

### **5.2 Variational Laws Explained**

A striking feature of fundamental physics: laws take variational form. Classical mechanics follows from the principle of least action. General relativity derives from the Einstein-Hilbert action. Quantum mechanics admits path-integral formulation. The Standard Model is defined by a Lagrangian. Fermat's principle governs optics; Hamilton's principle governs dynamics. Why should nature prefer variational principles?

IPP provides an answer that transforms this from brute fact to structural necessity.

Consider two ways to specify the same physics: (a) "Systems follow paths extremizing S \= ∫L dt" (compact variational principle); (b) exhaustive enumeration of permitted trajectories.

Specification (a) has vastly lower K(S). A single functional plus minimization procedure replaces enumeration of infinitely many trajectories. Under IPP, variational universes dominate. The Principle of Least Action is not brute fact but consequence of algorithmic parsimony at the level of structure-space.

This unifies three seemingly distinct parsimony principles:

**Ontological parsimony:** Simpler structures are more probable (IPP). Structures with lower Kolmogorov complexity dominate the measure.

**Dynamical parsimony:** Simpler paths are preferred (Principle of Least Action). Systems evolve along extremal trajectories rather than arbitrary paths.

**Epistemic parsimony:** Simpler theories should be believed (Occam's Razor). We prefer hypotheses with fewer free parameters.

All three reflect a single underlying truth: algorithmic simplicity is not merely guide to inference but feature of reality's deep structure. Occam's Razor works because it tracks ontological probability. The Principle of Least Action holds because variational laws minimize generative complexity.

Feynman's path integral provides additional insight. In quantum mechanics, all paths contribute to the propagator, weighted by e^(iS/ℏ). The classical path (extremal action) dominates because nearby paths constructively interfere while distant paths destructively cancel. The Principle of Least Action emerges as consequence of phase coherence. This is parsimony at yet another level.

### **5.3 Testable Predictions**

**Prediction 1:** The final theory should have near-minimal K(S) among observer-supporting theories. If the ToE requires irreducibly complex machinery (\~10^6 bits), IPP faces difficulty.

**Prediction 2:** Mathematical structures should describe physical reality with 'unreasonable effectiveness.'

**Prediction 3:** Complexity should be generated from simple rules, not fundamental.

**Prediction 4:** Cross-domain mathematical transfer should be pervasive.

**Prediction 5:** Outstanding physics puzzles will have simple solutions. Quantum gravity unification, dark matter, and dark energy currently lack complete explanations. IPP predicts their solutions will be simple: low K additions or modifications to existing frameworks, not baroque new structures. If quantum gravity requires 10^6 bits of irreducible specification, or dark matter requires dozens of new particle types with arbitrary parameters, IPP is challenged. The objection "of course solutions will be simple" concedes the point: the expectation of simplicity *is* IPP, operating as implicit assumption. Making it explicit transforms intuition into testable prediction.

Current physics is consistent with all five predictions.

Wolfram's Physics Project provides convergent evidence: the hypothesis that physics emerges from simple hypergraph rules (a few bits) aligns with IPP. Verification faces barriers at scales \~10^−93 meters, but mathematical derivation may bridge the gap. A precise irony: the simplest specification may require the most computation to verify, since discrete steps must be played out to macroscopic scales. This inverse relationship is not general (continuous formulas like F=ma yield predictions directly), but the Wolfram case is extreme in both directions.

Mathematics reveals why simple discrete structures yield simple continuous forms: both express identical underlying regularity. Infinite series with closed forms, symmetric groups with continuous representations, convergent sequences with analytic limits. In each case the discrete and continuous descriptions are informationally equivalent, having identical Kolmogorov complexity because they encode identical structure. IPP therefore predicts not merely simple laws but coherent multi-level simplicity: if a discrete substrate exists, it should admit simple continuous formulation.

A contrasting approach: Sandora (2025) argues habitability constraints may favor complex theories. This conflates K(laws) with K(outcomes). Simple laws routinely generate complex outcomes (Mandelbrot set, life from Standard Model). IPP predicts simple laws; this is a testable disagreement.

### **5.4 Anthropic Implications**

**Boltzmann brains:** IPP alone doesn't fully resolve this challenge, which requires distinguishing two levels of algorithmic probability.

*Level 1 (Inter-structure): IPP.* P(S) ∝ 2^(−K(laws)) determines weight of different mathematical structures.

*Level 2 (Intra-structure): Conditional algorithmic probability.* P(σ | S) ∝ 2^(−K(σ | laws)) determines which states are typical within a structure.

In thermal equilibrium universes, K(laws) is very low ("thermal equilibrium at temperature T"). However, an observer at time t₀ requires specifying an exact low-entropy configuration, giving high K(observer | laws). For persistence through times t₁, t₂, ..., tₙ, complexity grows linearly with n, yielding probability P ∝ 2^(−cn) exponentially suppressed in persistence time. Each moment requires "winning the lottery" again.

In structured universes (like ours), K(laws) is moderate (\~10³ bits), but observers emerge naturally from law-governed dynamics. K(observer | laws, IC) is low because observers are computable outputs. Persistence requires no additional specification; it is guaranteed by dynamics.

Even if thermal structures have higher IPP weight (simpler laws), structured observers dominate because P(observer | thermal) is exponentially suppressed while P(observer | structured) is high.

Additionally, the transcendental constraint applies: Boltzmann brains don't persist or learn from patterns. A momentary configuration representing having learned is not an observer but a static record. They may not qualify as observers at all.

**Simulations:** A simulation requires K(base physics) \+ K(computer) \+ K(code), strictly greater than base reality alone. Under IPP, simulations are exponentially suppressed. You should expect to inhabit base reality.

**Fermi paradox and temporal location:** Colonial empires require specifying expansion machinery, coordination protocols, specific observer locations. If K(colonial state | laws) \> K(solo state | laws), isolated civilizations dominate. Similarly, far-future observers require more specification (more entropy, more branches, more contingent history). Early observers have lower K(temporal location | laws). The framework renders these questions tractable though not decisively resolved; precise predictions require empirical modeling.

**Epistemic status:** Boltzmann brains and simulations are decisively resolved (exponential suppression via structural complexity arguments). Fermi paradox and temporal location are tractable but require empirical modeling for precise predictions. The two-level architecture (laws vs states) clarifies which arguments are possible.

### **5.5 Quantum Mechanics and Many-Worlds**

The quantum wavefunction is itself mathematical structure: a vector in Hilbert space evolving unitarily under the Schrödinger equation. On the present view, this is not representation of something else; it is the thing itself.

The Many-Worlds Interpretation coheres naturally with the framework. MWI adds nothing to the Schrödinger equation; collapse interpretations add non-unitary processes; hidden-variable theories add unobservable particles and pilot waves. Under IPP, the interpretation adding least to K(S) should be favored. Many-Worlds, requiring only unitary evolution, has minimal K and dominates the measure over quantum interpretations.

**The Born Rule:** Why do observed frequencies match |Ψ|²? This remains an active research area. The best current approaches derive it from Hilbert space geometry (the inner product naturally yields squared amplitudes as "lengths") combined with causal consistency constraints. Recent work suggests the quadratic form reflects deep information-theoretic requirements rather than arbitrary convention. For the present framework: the Born rule is mathematical structure, following from Hilbert space geometry. Whether we call it "derived" or "structural feature" is semantic; it is mathematics throughout. The IPP-compatible conjecture is that high-amplitude branches have lower K (less information needed to specify them), aligning algorithmic probability with Born probabilities. This remains plausible but unproven.

**Bell's Theorem as Corroboration:** Bell's theorem provides striking corroboration for mathematical structural realism. Entangled particles separated by arbitrary distances correlate their measurement outcomes more strongly than any local hidden-variable theory permits. These violations are experimentally confirmed to high precision.

Mathematical structural realism dissolves the puzzle. If reality *is* mathematical structure, and space is structure within mathematics (not the container of mathematics), then spatially separated particles need not 'signal' across distance. They are one mathematical pattern, described by a single entangled state vector. The correlations are not causal influences propagating through space; they are structural features of a unified mathematical object. Bell correlations are not 'spooky action at a distance'; they are mathematics.

**Clarification on interpretations:** MWI is favored by IPP for structures like ours, but the Level IV multiverse contains all consistent structures, including those where collapse is fundamental or Bell correlations arise through genuinely nonlocal hidden variables. All consistent quantum interpretations correspond to structures that exist. MWI dominates the measure; it is not the only option that exists.

### **5.6 Consciousness and the Hard Problem**

Consciousness represents a crucial test case. Chalmers argues that even complete physical knowledge leaves unexplained why there is something it is like to be a conscious system. This motivates property dualism: something beyond structure constitutes experience.

The response: the hard problem assumes its conclusion. To claim structure cannot constitute experience presupposes experience is not structural. But our argument establishes that anything existing and causally interacting must be structure. Consciousness causally interacts with physical systems (pain causes the report "that hurts"). Therefore consciousness is structure.

Crucially, any proposed explanation of consciousness must itself be mathematical. Penrose's Orch-OR invokes quantum coherence in microtubules. Integrated Information Theory defines consciousness via the mathematical quantity Φ. Global Workspace Theory describes computational broadcasting. Dualist proposals invoking "non-physical" properties must still specify *which* properties and *how* they interact, which is specifying structure. The pattern-randomness dichotomy applies: any determinate feature of consciousness, any mechanism proposed to explain it, falls within mathematical description. Objections to mathematical consciousness invariably invoke more mathematics. The escape routes all lead back.

Russellian monism offers a compatible perspective. Russell (1927) observed that physics describes only relational structure, leaving open what *instantiates* those relations. Russellians propose that phenomenal properties are the intrinsic nature of physical structure. On mathematical monism, this collapses elegantly: there is no gap between structure and its intrinsic nature because structure is fundamental. Qualia are not hidden fillers of structural nodes but aspects of what it is like to *be* certain mathematical patterns. The hard problem dissolves not by explaining qualia in terms of something else but by recognizing that mathematical structure, from the inside, constitutes experience.

---

## **6\. Objections and Responses**

**'This is untestable metaphysics.'** The objection conflates direct observation with testability. We cannot directly observe other structures, but IPP makes predictions about *this* structure that could be falsified. If the final theory has irreducibly high complexity, IPP is effectively falsified. If physical laws did not take variational form, IPP would be effectively falsified. The framework is testable through consequences for observable physics.

Many accepted theories posit unobservables: quarks cannot be isolated; wavefunctions cannot be directly measured; the early universe cannot be observed. Science routinely posits unobservables that explain observables. Additionally, quantum computing provides indirect evidence for Many-Worlds: superposition doing computational work suggests branches exist rather than being mere mathematical fiction.

**'IPP is ad hoc.'** IPP is not ad hoc but uniquely motivated. Algorithmic probability is the unique measure dominating all computable alternatives (Solomonoff's completeness result). It is not one arbitrary choice among many but the principled choice given information-theoretic foundations. Furthermore, IPP has independent support: it explains why Occam's Razor works. If simpler theories were not more likely true, parsimony's historical success would be miraculous.

**'Why this measure?'** IPP does not eliminate all 'why' questions but relocates them. We trade 'why these laws?' for 'why algorithmic probability?' The latter is more tractable: algorithmic probability has unique mathematical properties (dominance over computable measures, invariance under machine choice) that physical laws lack.

Some explanatory terminus is necessary. Every framework reaches a point where further 'why' questions become incoherent. IPP terminates at computation and information, fundamental concepts that may not admit further reduction. This is genuine bedrock, not arbitrary stopping.

**'The Gödel objection.'** Gödel's theorems show formal systems have unprovable truths. But these are still mathematical truths, just not provable within that system. Incompleteness shows mathematical truth is richer than any single formal system, not that anything escapes mathematics. We overcome Gödel limitations by shifting to richer mathematical frameworks, which face their own incompleteness. Undecidability is inherent to mathematical structure itself.

**'This is just a tautology.'** The objection: you define existence as mathematical structure, then conclude reality is mathematical structure. This is trivially true by definition.

The response: the objection misidentifies the argumentative goal. The claim is not 'structured reality is structured' but 'any coherent alternative to structural reality is incoherent.' The philosophical substance lies in demonstrating that apparent alternatives (substrate, haecceity, non-relational intrinsic properties) either collapse into structuralism or face decisive objections. We do not stipulate that reality is structure; we show that non-structure cannot coherently exist. This is substantive, not tautological.

**'The core premise is circular.'** The objection: you define existence via intelligibility, which presupposes minds. Is this not anthropocentric? The response: the objection conflates two senses of intelligibility. Intelligibility-as-structure means having determinate properties in definite relations; this is objective and observer-independent. Intelligibility-as-knowability means being graspable by minds; this is observer-dependent but is not our claim's foundation. We are not saying reality is mathematical because we understand it. We are saying anything with determinate properties constitutes structure, and structure is mathematical by definition.

---

## **7\. Conclusion**

The Level IV multiverse follows from the identity of existence with mathematical consistency. All self-consistent structures exist because any filter selecting among them would itself require grounding. The measure problem is solved by IPP: structures weighted by algorithmic probability 2^(−K(S)).

This explains fine-tuning (simple laws dominate), variational form of physical laws (minimal generative complexity), and Occam's Razor success (simplicity tracks truth because simple structures dominate measure). IPP generates testable predictions about theory structure.

Mathematical consistency does not require substrate, creator, or external grounding. It is self-necessitating. And because we are patterns within that consistency, we find ourselves, inevitably, in a patterned, comprehensible reality.

---

## **References**

Adams, R. M. (1979). Primitive thisness and primitive identity. *Journal of Philosophy*, 76(1), 5–26.

Benacerraf, P. (1973). Mathematical truth. *Journal of Philosophy*, 70(19), 661–679.

Black, M. (1952). The identity of indiscernibles. *Mind*, 61(242), 153–164.

Esfeld, M., & Lam, V. (2008). Moderate structural realism about space-time. *Synthese*, 160(1), 27–46.

French, S. (2014). *The Structure of the World*. Oxford University Press.

Ladyman, J., & Ross, D. (2007). *Every Thing Must Go: Metaphysics Naturalized*. Oxford University Press.

Lewis, D. (1986). *On the Plurality of Worlds*. Blackwell.

Sandora, M. (2025). Multiverse predictions for habitability. arXiv:2509.08220.

Russell, B. (1927). *The Analysis of Matter*. Kegan Paul.

Saunders, S. (2006). Are quantum particles objects? *Analysis*, 66(1), 52–63.

Schmidhuber, J. (1997). A computer scientist's view of life, the universe, and everything. *Lecture Notes in Computer Science*, 1337, 201–208.

Schmidhuber, J. (2000). Algorithmic theories of everything. arXiv:quant-ph/0011122.

Schmidhuber, J. (2002). The Speed Prior: A new simplicity measure yielding near-optimal computable predictions. *Proceedings of COLT 2002*, LNAI 2375, 216–228.

Solomonoff, R. J. (1964). A formal theory of inductive inference. *Information and Control*, 7(1–2), 1–22, 224–254.

Tegmark, M. (2008). The mathematical universe. *Foundations of Physics*, 38(2), 101–150.

Tegmark, M. (2014). *Our Mathematical Universe*. Knopf.

Wolfram, S. (2020). *A Project to Find the Fundamental Theory of Physics*. Wolfram Media.

